{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroDecisionTrees_EDS.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "eszKqy-1Lqif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Equity Data Science Educational Series\n",
        "\n",
        "### Monthly Data Science Lecture Series Part 1:\n",
        "### An Introduction to Decision Trees, Random Forests, and Gradient Boosting Machines"
      ]
    },
    {
      "metadata": {
        "id": "nVaouvV5LzUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All examples from today's EDS educational session are included within this Jupyter Notebook. For those new to Jupyter Notebooks, code blocks can be executed by selecting the desired cell and pressing 'shift+enter', or clicking the 'play' button that appears at the upper left of the block.\n",
        "\n",
        "The ouput from executing the code contained within the cell will then appear immediately below the executed cell. Give the simple example below a try:"
      ]
    },
    {
      "metadata": {
        "id": "p_Ck01J1MH7B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Jupyter Test - Welcome to the EDS Educational Series on Wednesday, August\", 2**5-10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNZOXPCUoh00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Before we begin, please take a moment to install a visualization engine by executing the below cell:*"
      ]
    },
    {
      "metadata": {
        "id": "CjmEWyL-ogsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install graphviz \n",
        "!apt-get install graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LS7iPV4YMSCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "### The following topics will be discussed during today's session:\n",
        "\n",
        "    > Decision Tree Modeling\n",
        "        > Decision Tree Classification\n",
        "        > Decision Tree Regression\n",
        "    > Decision Tree Ensemble Methods\n",
        "        > Random Forest\n",
        "        > Gradient Boosting Machines"
      ]
    },
    {
      "metadata": {
        "id": "Xgdhjyq5Q5sV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code examples for today's session will utilize a dataset of median home prices across neighborhoods in the Boston Metropolitan area that is a common benchmark for algorithms in the machine learning community. Let's take a look at the data:"
      ]
    },
    {
      "metadata": {
        "id": "zeXkYS95RBg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# load dataset\n",
        "boston = load_boston()\n",
        "housing = pd.DataFrame(boston.data)\n",
        "housing['Median_Val'] = boston.target\n",
        "housing.columns = ['Crime_Rate', 'Prop_Zoned', 'Prop_NonRetail', 'Borders_River',\n",
        "                   'Conc_NO2', 'Avg_Rooms','Prop_Pre1940', 'Dist_Employ', \n",
        "                   'Hway_Access', 'Property_Tax', 'TeachStu_Ratio', 'B', \n",
        "                   'LSTAT', 'Median_Val']\n",
        "\n",
        "# Dropping a few columns to reduce dimensions\n",
        "drop_cols = ['B', 'LSTAT', 'Property_Tax']\n",
        "housing = housing[[a_col for a_col in housing.columns if not a_col in drop_cols]]\n",
        "\n",
        "# Print first 10 rows of the housing data set\n",
        "housing.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcygicgiRmh3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to also utilize the housing data to explore the application of decision trees to classification problems, we will add a categorical target that splits the median value into terciles for use in our decision tree classifiers:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zZeR5nyQRrb8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create categorical version of response variable [0 = lowest, 1 = middle, 2 = high]\n",
        "housing['Val_Cat'] = pd.qcut(housing.Median_Val,3,labels = [\"Low\",\"Medium\",\"High\"])\n",
        "housing.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-CuX1HwR33Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As our objective is to build a predictive model that will work out of sample, let's split the dataset into a training, validation, and test set to gauge predictive accuracy."
      ]
    },
    {
      "metadata": {
        "id": "ppHraef5RrbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summary statistics for variables\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into two groups: 80% train + validation, 20% test\n",
        "housing_train, housing_test = train_test_split(housing, test_size = 0.2, random_state = 1991)\n",
        "\n",
        "# split train data into two further groups (train + validation)\n",
        "housing_train, housing_valid = train_test_split(housing_train, test_size = 0.2, random_state = 1991)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_od_kp8eR-_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We now have three data sets, 323 training data points, 81 validation data points, and 102 test data points\n",
        "print(\"Training Data: Number of Rows, Number of Columns:\", housing_train.shape)\n",
        "print(\"Validation Data: Number of Rows, Number of Columns:\",housing_valid.shape)\n",
        "print(\"Testing Data: Number of Rows, Number of Columns:\",housing_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWj1XHWyR8NS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a quick peak at the summary statistics and correlations among variables in the training data:"
      ]
    },
    {
      "metadata": {
        "id": "2-CllfNESJRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# display summary statistics for training data\n",
        "housing_train.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYCPxo6dStwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# correlation matrix of variables\n",
        "import seaborn as sborn\n",
        "color_map= sborn.diverging_palette(5, 250, as_cmap=True)\n",
        "feature_cor = housing_train.corr()\n",
        "feature_cor.style.background_gradient(color_map, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5YyDECpS7FW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before beginning any predictive modeling exercise, it is useful to:\n",
        "\n",
        "    > Establish Measure of Model Performance\n",
        "      > Mean Squared Error\n",
        "    > Generate Lazy Performance Baseline\n",
        "      > Average Value\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "q9NdGLL4S-0k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function to compute mean squared error\n",
        "def mean_squared_error(predicted_values, actual_values):\n",
        "    return np.mean((predicted_values - actual_values) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0zoqH_6blLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot mean-squared error\n",
        "\n",
        "# generate a sequence from -5 to 5\n",
        "my_seq = np.linspace(18,28,100)\n",
        "\n",
        "# plot mse if true value is 23\n",
        "my_error = [mean_squared_error(pred_val, 23) for pred_val in my_seq]\n",
        "\n",
        "# generate plot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(my_seq, my_error)\n",
        "plt.ylabel('Squared Error')\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.title('Squared Error by Predicted Value - if True Value is 23')\n",
        "plt.axvline(x=23, color = 'red')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJNArDcnTGVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Decision Tree Modeling"
      ]
    },
    {
      "metadata": {
        "id": "zzCzPwLXTIKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Decision trees are a class of nonpametric models that partition datasets into sub-regions.\n",
        "\n",
        "In predictive modeling tasks, predictions for new observations are generated by following the rules of the tree from root to leaf.\n",
        "\n",
        "Let's take a look at a quick example. \n",
        "\n",
        "To simplify, we will start by only using two independent variables (*Average # Rooms* and *Distance to Employment Centers*), to predict which tercile the value falls in (categorical response)."
      ]
    },
    {
      "metadata": {
        "id": "fzOBpMyWTLBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import decision tree building functions and visualization functions from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import display, SVG\n",
        "from graphviz import Source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xueGkuK4USd6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fit a basic decision tree classifier on the training data\n",
        "import os\n",
        "\n",
        "ind_vars =['Avg_Rooms', 'Dist_Employ']\n",
        "# specify that tree must have depth of no more than 2\n",
        "classifier = DecisionTreeClassifier(max_depth = 2)\n",
        "# sample first 50 observations from the training set\n",
        "classifier.fit(housing_train.head(50)[ind_vars], housing_train.head(50).Val_Cat)\n",
        "\n",
        "# Plot the tree\n",
        "tree_plot = Source(tree.export_graphviz(classifier, out_file = None, \n",
        "                        feature_names = ind_vars, class_names = ['Low', 'Medium', 'High'],\n",
        "                        filled = True))\n",
        "display(SVG(tree_plot.pipe(format = 'svg')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1HN03ivhJxZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above decision tree can be re-written as a series of simple if-then decision rules:\n",
        "\n",
        "**IF** the average number of rooms in the neighborhood is less than (or equal to) **6.517** **THEN**\n",
        "> **IF** the distance from employment index is less than (or equal to) **2.505** **THEN**\n",
        "\n",
        ">> We predict \"**Low**\" value\n",
        "\n",
        "> **IF** the distance from employment index is greater than **2.505** **THEN**\n",
        "\n",
        ">> We predict \"**Medium**\" value\n",
        "  \n",
        "**IF** the average number of rooms in the neighborhood is greater than **6.517** **THEN**\n",
        "> **IF** the distance from employment index is less than (or equal to) **2.565** **THEN**\n",
        "\n",
        ">> We predict \"**High**\" value\n",
        "\n",
        "> **IF** the distance from employment index is greater than **2.565** **THEN**\n",
        "\n",
        ">> We predict \"**High**\" value (*but with greater confidence*)"
      ]
    },
    {
      "metadata": {
        "id": "xBtI1nzChJvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "s3Cg7XvKkySg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot the tree again\n",
        "display(SVG(tree_plot.pipe(format = 'svg')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD3KX6GFk2Dh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The decision tree structure in the above plot was constructed by minimizing the Gini Impurity in a greedy fashion:\n",
        "\n",
        "Gini Impurity: 1 - [prob(Low)^2 + prob(Medium)^2  + prob(High)^2]\n",
        "\n",
        "For the bottom left node, for example, the Gini impurity is: 1-[(15/18)^2 + (3/18)^2 + (0/18)^2] = 0.278\n",
        "\n",
        "Below, we plot the Gini impurity by split for the first decision in the tree to illustrate:\n"
      ]
    },
    {
      "metadata": {
        "id": "5Q5xpw9ksxQz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define function to compute gini impurity\n",
        "from collections import Counter\n",
        "def compute_gini(resp_var):\n",
        "  my_list = [((val * 1.0)/len(resp_var))**2 for val in Counter(resp_var).values()]\n",
        "  gini = 1-np.sum(my_list)\n",
        "  return gini\n",
        "\n",
        "\n",
        "# define function to compute gini criterion for each possible split point\n",
        "def create_gini_cutoffs(ind_var, resp_var):\n",
        "  ind_var = list(ind_var)\n",
        "  resp_var = list(resp_var)\n",
        "  hold_values = []\n",
        "  hold_impurity = []\n",
        "  unique_vals = sorted(list(set(ind_var)))\n",
        "  for a_val in unique_vals:\n",
        "    if a_val < np.max(unique_vals):\n",
        "      less_than_equal = [i for i, val in enumerate(ind_var) if val <= a_val]\n",
        "      greater_than = list(set(list(range(0,50))) - set(less_than_equal))\n",
        "      less_equal_resp = [val for i, val in enumerate(resp_var) if i in less_than_equal ]\n",
        "      greater_resp = [val for i, val in enumerate(resp_var) if i in greater_than]\n",
        "      impurity_less = compute_gini(less_equal_resp)\n",
        "      impurity_greater = compute_gini(greater_resp)\n",
        "      hold_values.append(a_val)\n",
        "      hold_impurity.append(impurity_less * len(less_than_equal)/len(resp_var) + impurity_greater * len(greater_than)/len(resp_var))\n",
        "  return hold_values, hold_impurity\n",
        "    \n",
        "ind_vars =['Avg_Rooms', 'Dist_Employ']\n",
        " \n",
        "vals_room, gini_room = create_gini_cutoffs(housing_train.head(50)['Avg_Rooms'],housing_train.head(50)['Val_Cat'])\n",
        "vals_dist, gini_dist = create_gini_cutoffs(housing_train.head(50)['Dist_Employ'],housing_train.head(50)['Val_Cat'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uc9Lj0DssxPV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot the gini impurity by splitpoint\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize = (20,5))\n",
        "ax1.plot(vals_room, gini_room, linestyle=\"--\", marker = 'o', color='b')\n",
        "ax1.set_title(\"Gini Impurity by Value of Average Rooms\")\n",
        "ax1.set_xlabel(\"Average Rooms\")\n",
        "ax1.set_ylabel(\"Gini Impurity\")\n",
        "ax2.plot(vals_dist,gini_dist, linestyle=\"--\", marker = 'o', color='g')\n",
        "ax2.set_title(\"Gini Impurity by Value of Distance to Employment\")\n",
        "ax2.set_xlabel(\"Distance to Employment\")\n",
        "ax1.axvline(x=6.517, color = 'red')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Us_o-4RN-DEe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A simplified decision tree algorithm:\n",
        "\n",
        "For every feature (here Average Rooms and Distance to Employment):\n",
        "> Calculate the loss at each split point\n",
        "Choose the feature and split point that minimizes the loss function (Gini impurity)\n",
        "\n",
        "Repeat this process recursively on the 2 datasets resulting from this split\n",
        "\n",
        "Keep going until either:\n",
        "> Tree has no loss (default)\n",
        "\n",
        "> Minimum number of observations allowed per leaf threshold is reached\n",
        "\n",
        "> Maximum depth of tree allowed is reached"
      ]
    },
    {
      "metadata": {
        "id": "vzgiG0Z5_H_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now visualize the prediction surface that this decision tree is creating, and see how chaning the tuning parameters affects the model.\n",
        "\n",
        "Note that to make the visualization more manageable, I have randomly selected only 150 values from the original training set. And again we will only use our 2 variables for the time being."
      ]
    },
    {
      "metadata": {
        "id": "PcMAjVFnUh-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# subset of the training data to make visualization more manageable\n",
        "train_sub1 = housing_train.sample(n=150, random_state=1991, replace=False)\n",
        "train_sub1_x = train_sub1[['Avg_Rooms', 'Dist_Employ']]\n",
        "train_sub1_y = train_sub1.Val_Cat\n",
        "plt.scatter(train_sub1_x.Avg_Rooms, train_sub1_x.Dist_Employ, c=train_sub1_y, cmap = 'rainbow')\n",
        "plt.title('Average Rooms vs. Distance to Employment')\n",
        "plt.xlabel('Average Rooms')\n",
        "plt.ylabel('Distance to Employment')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jB7D-JeveFEh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tree(max_depth, min_leaf = 0.2):\n",
        "    estimator = DecisionTreeClassifier(random_state = 1991,\n",
        "                                      max_depth = max_depth,\n",
        "                                      min_samples_leaf = min_leaf)\n",
        "    estimator = estimator.fit(train_sub1_x,train_sub1_y)\n",
        "    \n",
        "    graph = Source(tree.export_graphviz(estimator, out_file = None, feature_names = train_sub1_x.columns, \n",
        "                                       class_names = ['0', '1', '2'],\n",
        "                                       filled = True))\n",
        "    #plot_tree_boundaries(max_depth, train_sub2_x, train_sub2_y)\n",
        "    display(SVG(graph.pipe(format = 'svg')))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwgeMg1-eK8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Try changing the tree tuning parameters. { run: \"auto\" }\n",
        "max_depth = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "min_leaf = 0.06 #@param {type:\"slider\", min:0.01, max:1, step:0.05}\n",
        "\n",
        "plot_tree(max_depth, min_leaf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mTj6gqRMUndE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tree_boundaries(max_depth, data_x, data_y, par = None, cmap = 'rainbow'):\n",
        "    if par is None:\n",
        "        par = plt.gca()\n",
        "        \n",
        "    # plot the training points\n",
        "    par.scatter(data_x.values[:,0], data_x.values[:,1], c = data_y, s = 50, cmap = cmap, clim = (data_y.min(), data_y.max()), zorder = 3)\n",
        "    par.axis('tight')\n",
        "    #par.axis('off')\n",
        "    par.set_xlabel('Average Rooms')\n",
        "    par.set_ylabel('Distance to Employment')\n",
        "    par.set_title(\"Decision Tree Prediction Surface\")\n",
        "    lim_x = par.get_xlim()\n",
        "    lim_y = par.get_ylim()\n",
        "    \n",
        "    classifier = DecisionTreeClassifier(max_depth = max_depth)\n",
        "    classifier.fit(data_x, data_y)\n",
        "    mesh_x, mesh_y = np.meshgrid(np.linspace(*lim_x, num = 200), np.linspace(*lim_y, num = 200))\n",
        "    mesh_preds = classifier.predict(np.c_[mesh_x.ravel(), mesh_y.ravel()]).reshape(mesh_x.shape)\n",
        "    \n",
        "    n_class = len(np.unique(data_y))\n",
        "    mesh_colors = par.contourf(mesh_x, mesh_y, mesh_preds, alpha = 0.3, levels = np.arange(n_class + 1)-0.5, \n",
        "                               cmap=cmap, clim = (data_y.min(), data_y.max()), zorder = 1)\n",
        "    \n",
        "    par.set(xlim = lim_x, ylim = lim_y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pP2XyrHvAZx9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is the decision boundary for a tree of maximum depth 8. There appear to be some regions where the prediction surface is relatively noisy. "
      ]
    },
    {
      "metadata": {
        "id": "9He04W7SUroO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_tree_boundaries(8, train_sub1_x, train_sub1_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yUAAyhmXA_Bl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What happens if we take a new random subsample from the dataset? Do we get a similar decision surface?"
      ]
    },
    {
      "metadata": {
        "id": "lPD6AgX7U0Cc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# subset of the training data to make visualization more manageable\n",
        "train_sub2 = housing_train.sample(n=150, random_state=1992, replace=False)\n",
        "train_sub2_x = train_sub2[['Avg_Rooms', 'Dist_Employ']]\n",
        "train_sub2_y = train_sub2.Val_Cat\n",
        "plot_tree_boundaries(8, train_sub2_x, train_sub2_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "645LmIU2BHi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our decision tree appears to be overfitting - two trees built from random subsamples of the dataset produce quite different prediction surfaces. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "edPjCcgWhr9z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Decision Tree Ensemble Methods\n",
        "\n",
        "## Random Forests"
      ]
    },
    {
      "metadata": {
        "id": "eJodMAx1Bcds",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Earlier, we found that building decision trees on two different random subsamples produced two different decision surfaces.\n",
        "\n",
        "The decision surfaces were the same in some areas, and different in others. What happens if we average the decision surfaces across the trees?"
      ]
    },
    {
      "metadata": {
        "id": "60a_SBwlhtaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhuRbfZthvUH",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "def plot_random_forest(n_estimators=1):\n",
        "            \n",
        "    data_x = train_sub2_x\n",
        "    data_y = train_sub2_y\n",
        "    par = None\n",
        "    cmap = 'rainbow'\n",
        "    max_depth = 1\n",
        "    min_leaf = 0.2\n",
        "    \n",
        "    if par is None:\n",
        "        par = plt.gca()\n",
        "\n",
        "    par.scatter(data_x.values[:,0], data_x.values[:,1], c = data_y, s = 50, cmap = cmap, clim = (data_y.min(), data_y.max()), zorder = 3)\n",
        "    par.axis('tight')\n",
        "    #par.axis('off')\n",
        "    lim_x = par.get_xlim()\n",
        "    lim_y = par.get_ylim()\n",
        "    \n",
        "    rf_estimator = RandomForestClassifier(random_state = 1991,\n",
        "                                      n_estimators = n_estimators,\n",
        "                                      min_samples_leaf =2)\n",
        "    rf_estimator.fit(train_sub2_x,train_sub2_y)\n",
        "    \n",
        "    mesh_x, mesh_y = np.meshgrid(np.linspace(*lim_x, num = 200), np.linspace(*lim_y, num = 200))\n",
        "    mesh_preds = rf_estimator.predict(np.c_[mesh_x.ravel(), mesh_y.ravel()]).reshape(mesh_x.shape)\n",
        "    \n",
        "    n_class = len(np.unique(data_y))\n",
        "    mesh_colors = par.contourf(mesh_x, mesh_y, mesh_preds, alpha = 0.3, levels = np.arange(n_class + 1)-0.5, \n",
        "                               cmap=cmap, zorder = 1)\n",
        "    \n",
        "    par.set(xlim = lim_x, ylim = lim_y) \n",
        "    par.set_xlabel('Average Rooms')\n",
        "    par.set_ylabel('Distance to Employment')\n",
        "    par.set_title(\"Decision Tree Prediction Surface\")\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5D5QTTv_sK8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we observe changes to the decision surface as we continue to average together more decision trees. "
      ]
    },
    {
      "metadata": {
        "id": "7oE97IzQsMj9",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the decision surface changes as we add more trees. { run: \"auto\" }\n",
        "n_estimators = 100 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "plot_random_forest(n_estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnnpNhbVDG0a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Random Forest Algorithm:\n",
        "\n",
        "\n",
        "**RANDOMLY select observations (rows) from the dataset with replacement**\n",
        "\n",
        "For a **RANDOM subset of features** (here Average Rooms and Distance to Employment):\n",
        "> Calculate the loss at each split point\n",
        "\n",
        "Choose the feature and split point that minimizes the loss function (Gini impurity)\n",
        "\n",
        "Repeat this process recursively on the 2 datasets resulting from this split\n",
        "\n",
        "Keep going until either:\n",
        "> Tree has no loss (default)\n",
        "\n",
        "> Minimum number of observations allowed per leaf threshold is reached\n",
        "\n",
        "> Maximum depth of tree allowed is reached\n",
        "\n",
        "**Build many of these trees, and average their predictions together**"
      ]
    },
    {
      "metadata": {
        "id": "1nBnCwY9sK7i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "metadata": {
        "id": "P-jKdpm3EDVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Rather than building a number of weak trees and averaging them as in random forests, here we build trees sequentially. Each tree learns a little from the mistakes of the trees that came before it. As we continue to add trees, our model grows stronger (until eventually it reaches a point at which it begins to overfit).\n",
        "\n",
        "\n",
        "\n",
        "*Gradient Boosting Algorithm*\n",
        "\n",
        "1) Build a simple decision tree. \n",
        "\n",
        "2) Generate predictions from the tree.\n",
        "\n",
        "3) Multiply the predictions by a learning rate fraction (0< learning rate < 1)\n",
        "\n",
        "4) Compute the errors resulting from these predictions\n",
        "\n",
        "5) Build a new decision tree on the errors from part 4\n",
        "\n",
        "6) Repeat steps 1-5 until the ensemble begins to overfit\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jyNgHEGuFiT8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's move back to regression (predicting the median neighborhood values rather than the tercile), and check out a gradient boosting ensemble in action.\n",
        "\n",
        "First, we will use only a random sample of 150 observations, and only a single predictor \"Average Rooms\"."
      ]
    },
    {
      "metadata": {
        "id": "5924ytvatThS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBzAhY0itXTC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# subset of the training data to make visualization more manageable\n",
        "boosting_subset = housing_train.sample(n=150, random_state=1991, replace=False)\n",
        "boosting_subset = boosting_subset.sort_values('Avg_Rooms')\n",
        "boosting_subset_x = boosting_subset['Avg_Rooms']\n",
        "boosting_subset_y = boosting_subset['Median_Val']\n",
        "plt.scatter(boosting_subset_x, boosting_subset_y, c=boosting_subset_y, cmap = 'rainbow')\n",
        "plt.title('Median Value vs. Average # Rooms')\n",
        "plt.xlabel('Average # Rooms')\n",
        "plt.ylabel('Median Value')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AAc_lskRteXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_error = np.empty(100) * np.nan\n",
        "valid_error = np.empty(100) * np.nan\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iKnmmEGtXR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_gradient_boosting(num_trees=1, learn_rate = 0.30):\n",
        "    \n",
        "    data_x = boosting_subset_x.values.reshape(-1, 1)\n",
        "    data_y = boosting_subset_y\n",
        "    cmap = 'rainbow'\n",
        "\n",
        "    gradient_boosting_estimator = GradientBoostingRegressor(random_state = 1991,\n",
        "                                      n_estimators = num_trees,\n",
        "                                      learning_rate = learn_rate,\n",
        "                                      min_samples_leaf = 2,\n",
        "                                      max_depth = 1)\n",
        "    gradient_boosting_estimator = gradient_boosting_estimator.fit(data_x, data_y)\n",
        "\n",
        "    gradient_boosting_predictions = gradient_boosting_estimator.predict(data_x)\n",
        "    gradient_boosting_residuals = data_y - gradient_boosting_predictions\n",
        "    \n",
        "    # generate predictions on validation dataset\n",
        "    validation_predictions = gradient_boosting_estimator.predict(housing_valid['Avg_Rooms'].values.reshape(-1,1))\n",
        "    train_mse = mean_squared_error(boosting_subset_y, gradient_boosting_predictions)\n",
        "    valid_mse = mean_squared_error(housing_valid['Median_Val'].values, validation_predictions)\n",
        "    train_error[num_trees-1] = train_mse\n",
        "    valid_error[num_trees-1] = valid_mse\n",
        "    \n",
        "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, figsize = (20,5))\n",
        "    ax1.scatter(data_x, gradient_boosting_predictions, c = gradient_boosting_predictions, s = 50, cmap = cmap)\n",
        "    ax1.set_title('Gradient Boosting Predicted Values - ' +  str(num_trees) + ' iteration(s)')\n",
        "    ax1.axis((3.5,8.5,0,55))\n",
        "    ax2.scatter(data_x, gradient_boosting_residuals, c = np.abs(gradient_boosting_residuals), s = 50, cmap = cmap)\n",
        "    ax2.set_title('Gradient Boosting Residuals - ' +  str(num_trees) + ' iteration(s)')\n",
        "    ax2.axis((3.5,8.5,-30,30))\n",
        "    ax3.plot(range(0,num_trees), train_error[0:num_trees], label = 'train')\n",
        "    ax3.plot(range(0,num_trees), valid_error[0:num_trees], label = 'valid')\n",
        "    ax3.set_title('Gradient Boosting Validation Error - ' + str(num_trees) + ' iteration(s)')\n",
        "    ax3.legend(loc = 'upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45K9vx10tjbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the predictons and error change as we add more trees. { run: \"auto\" }\n",
        "num_trees = 18 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "learn_rate = 0.25 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "\n",
        "plot_gradient_boosting(num_trees, learn_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YuxAgnA0vw6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Minimum error of ', np.nanmin(valid_error), ' reached after ', np.nanargmin(valid_error)+1, 'iterations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rcXQr7KYtjaA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reset training and validation error\n",
        "train_error = np.empty(100) * np.nan\n",
        "valid_error = np.empty(100) * np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ek0PKbnG6FU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try again with a much higher learning rate"
      ]
    },
    {
      "metadata": {
        "id": "rOH8jOtLxNak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the predictons and error change as we add more trees. { run: \"auto\" }\n",
        "num_trees = 8 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "learn_rate = 0.96 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "\n",
        "plot_gradient_boosting(num_trees, learn_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOyJ8ahcxndm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Minimum error of ', np.nanmin(valid_error), ' reached after ', np.nanargmin(valid_error)+1, 'iterations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H5bnbsMGsacd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparison of Model Performance\n"
      ]
    },
    {
      "metadata": {
        "id": "m3Z5IDkQsfeE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that the underlying model approaches have been introduced, we will now select the 'best' model for this data set by analyzing performance on the validation data set. This champion model will then be utilized to generate predictions on the unseen test data set for a more complete picture of 'out of sample' performance."
      ]
    },
    {
      "metadata": {
        "id": "SqyMQRpdw7dR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictor_vars = ['Crime_Rate', 'Prop_Zoned', 'Prop_NonRetail', 'Borders_River',\n",
        "                   'Conc_NO2', 'Avg_Rooms','Prop_Pre1940', 'Dist_Employ', \n",
        "                   'Hway_Access', 'TeachStu_Ratio']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "md7ZPDgZHgXo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will try building decision trees with varying parameters of maximum depth and leaf size, and evaluate which combination looks most promising on the validation data. "
      ]
    },
    {
      "metadata": {
        "id": "agIh1429t3Y9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Decision Trees with Different Maximum Depths / Minimum Leaf Sizes\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "maximum_tree_depths = [1,3,5,7,9]\n",
        "minimum_tree_leaf_sizes = [1,5,10,15]\n",
        "\n",
        "tree_results = pd.DataFrame(list(product(maximum_tree_depths, minimum_tree_leaf_sizes)), columns=['Max_Depth', 'Leaf_Size'])\n",
        "tree_results['MSE'] = np.NAN\n",
        "\n",
        "tree_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xRP7185wC7r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for a_row in range(len(tree_results)):\n",
        "  \n",
        "  # build a decision tree\n",
        "  decision_tree = DecisionTreeRegressor(max_depth = tree_results['Max_Depth'][a_row],\n",
        "                                       min_samples_leaf = tree_results['Leaf_Size'][a_row],\n",
        "                                       random_state = 1991)\n",
        "  \n",
        "  # fit the decision tree\n",
        "  decision_tree = decision_tree.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "  # generate predictions\n",
        "  tree_predictions = decision_tree.predict(housing_valid[predictor_vars])\n",
        "  # generate error\n",
        "  tree_error = mean_squared_error(tree_predictions, housing_valid['Median_Val'])\n",
        "  # fill in column in test results\n",
        "  tree_results.loc[a_row, 'MSE'] = tree_error\n",
        "  \n",
        "tree_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6OrdbucvHvpi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The decision tree with lowest validation error had a maximum depth of 5 and minimum leaf size of 5. \n",
        "\n",
        "We wil now run a similar exercise for different parameter values of maximum depth, minimum leaf size, and number of trees in the ensemble. "
      ]
    },
    {
      "metadata": {
        "id": "7L1anBQv5PwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Random Forests with Different Maximum Depths / Minimum Leaf Sizes / Number of Trees\n",
        "\n",
        "maximum_rf_depths = [5,1000]\n",
        "minimum_rf_leaf_sizes = [1,5,10]\n",
        "number_rf_trees = [50,100]\n",
        "\n",
        "rf_results = pd.DataFrame(list(product(maximum_rf_depths, minimum_rf_leaf_sizes, number_rf_trees)), columns=['Max_Depth', 'Leaf_Size', 'Num_Trees'])\n",
        "rf_results['MSE'] = np.NAN\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfVOCa4M0S3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for a_row in range(len(rf_results)):\n",
        "  \n",
        "  # build a random forest\n",
        "  random_forest = RandomForestRegressor(max_depth = rf_results['Max_Depth'][a_row],\n",
        "                                       min_samples_leaf = rf_results['Leaf_Size'][a_row],\n",
        "                                       n_estimators = rf_results['Num_Trees'][a_row],\n",
        "                                       random_state = 1991)\n",
        "  \n",
        "  # fit the random forest\n",
        "  random_forest = random_forest.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "  # generate predictions\n",
        "  rf_predictions = random_forest.predict(housing_valid[predictor_vars])\n",
        "  # generate error\n",
        "  rf_error = mean_squared_error(rf_predictions, housing_valid['Median_Val'])\n",
        "  # fill in column in test results\n",
        "  rf_results.loc[a_row, 'MSE'] = rf_error\n",
        "  \n",
        "rf_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MeuEat-jIIwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The best parameters for the random forest on the validation set were a max_depth of 1000, min leaf size of 1, and 100 treees. \n",
        "\n",
        "On to gradient boosting, where we will vary the max depth, number of trees, and learning rate:"
      ]
    },
    {
      "metadata": {
        "id": "tTN8Y3EE7rS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Gradient Boosting with Different Maximum Depths / Number of Trees / Learn Rates\n",
        "\n",
        "maximum_gb_depths = [1,7]\n",
        "number_gb_trees = [30,300]\n",
        "gb_learn_rate = [.05,.3,]\n",
        "\n",
        "gb_results = pd.DataFrame(list(product(maximum_gb_depths, number_gb_trees, gb_learn_rate)), columns=['Max_Depth', 'Num_Trees', 'Learn_Rate'])\n",
        "gb_results['MSE'] = np.NAN\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBu7ULd588Xx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for a_row in range(len(gb_results)):\n",
        "  \n",
        "  # build a gradient booster\n",
        "  gradient_boost = GradientBoostingRegressor(max_depth = gb_results['Max_Depth'][a_row],\n",
        "                                       n_estimators = gb_results['Num_Trees'][a_row],\n",
        "                                       learning_rate = gb_results['Learn_Rate'][a_row],\n",
        "                                       min_samples_leaf = 5,\n",
        "                                       random_state = 1991)\n",
        "  \n",
        "  # fit the gradient booster\n",
        "  gradient_boost = gradient_boost.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "  # generate predictions\n",
        "  gb_predictions = gradient_boost.predict(housing_valid[predictor_vars])\n",
        "  # generate error\n",
        "  gb_error = mean_squared_error(gb_predictions, housing_valid['Median_Val'])\n",
        "  # fill in column in test results\n",
        "  gb_results.loc[a_row, 'MSE'] = gb_error\n",
        "  \n",
        "gb_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0kaBCWL0EL5P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The best gradient boosting ensemble (on the validation set) was constructed using 300 trees with a max depth of 7 and 0.05 learning rate.\n",
        "\n",
        "\n",
        "We will now retrain each model type utilizing the optimal parameters identified in the validation procedure above. "
      ]
    },
    {
      "metadata": {
        "id": "WLm1PUK5-Itq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# decision tree\n",
        "decision_tree = DecisionTreeRegressor(max_depth = 5,\n",
        "                                     min_samples_leaf = 5,\n",
        "                                     random_state = 1991)\n",
        "decision_tree = decision_tree.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "\n",
        "# random forest\n",
        "random_forest = RandomForestRegressor(max_depth = 1000,\n",
        "                                     min_samples_leaf = 1,\n",
        "                                     n_estimators = 100,\n",
        "                                     random_state = 1991)\n",
        "random_forest = random_forest.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "\n",
        "# gradient boosting\n",
        "gradient_boost = GradientBoostingRegressor(max_depth = 7,\n",
        "                                     n_estimators = 300,\n",
        "                                     learning_rate = 0.05,\n",
        "                                     random_state = 1991,\n",
        "                                     min_samples_leaf = 5)\n",
        "\n",
        "# fit the gradient booster\n",
        "gradient_boost = gradient_boost.fit(housing_train[predictor_vars], housing_train['Median_Val'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66aXbY12EWPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now generate predictions (along with a simple 'average' baseline) to evaluate how well the 'best' models identified through the validation procedure perform on the unseen test data."
      ]
    },
    {
      "metadata": {
        "id": "z5pgNNG8EzSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# simple average predictions\n",
        "avg_predictions = [housing_train['Median_Val'].mean()] * len(housing_test)\n",
        "# decision tree predictions\n",
        "tree_predictions = decision_tree.predict(housing_test[predictor_vars])\n",
        "# random forest predictions\n",
        "rf_predictions = random_forest.predict(housing_test[predictor_vars])\n",
        "# gradient boosting predictions\n",
        "gb_predictions = gradient_boost.predict(housing_test[predictor_vars])\n",
        "\n",
        "\n",
        "# errors across the models\n",
        "avg_error = mean_squared_error(avg_predictions, housing_test['Median_Val'])\n",
        "tree_error = mean_squared_error(tree_predictions, housing_test['Median_Val'])\n",
        "rf_error = mean_squared_error(rf_predictions, housing_test['Median_Val'])\n",
        "gb_error = mean_squared_error(gb_predictions, housing_test['Median_Val'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FoybXZRcGaT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Test Error Using Average from Training Set:', avg_error)\n",
        "print('Test Error Using Decision Tree:',tree_error)\n",
        "print('Test Error Using Random Forest:',rf_error)\n",
        "print('Test Error Using Gradient Boosting:',gb_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgXuG8-0N--v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Barplot\n",
        "result_height = [avg_error, tree_error, rf_error, gb_error]\n",
        "result_bars = ('Average', 'Tree', 'RandomForest', 'GradientBoosting')\n",
        "\n",
        "# Create bars\n",
        "plt.bar(result_bars, result_height)\n",
        "\n",
        "# add in a title\n",
        "plt.title('Test Error by Model' )\n",
        "plt.ylabel('Mean Squared Error')\n",
        "\n",
        "# Show graphic\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQFOd86CIuGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All models beat our lazy benchmark by some distance. Consistent with validation performance, the gradient boosting ensemble exhibited the strongest test performance, followed by the random forest. "
      ]
    },
    {
      "metadata": {
        "id": "a6KG6HzRQTPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extracting Insight from the Selected Model"
      ]
    },
    {
      "metadata": {
        "id": "S3JmVDC_Q2Cj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have selected our champion model and confirmed its efficacy utilizing the out of sample test data, we will attempt to leverage additional tools to better understand the insights that drive its predictions."
      ]
    },
    {
      "metadata": {
        "id": "MDG7pGiHRlen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# variable importances from model\n",
        "gb_importances = gradient_boost.feature_importances_\n",
        "\n",
        "# Plot the feature importances of the ensemble\n",
        "plt.figure()\n",
        "plt.title(\"Gradient Boosting Feature importances\")\n",
        "plt.barh(predictor_vars, gb_importances, color=\"r\", align=\"center\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "pd.DataFrame({'Feature' : predictor_vars, 'Importance': gb_importances}).sort_values(\"Importance\", ascending = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_L1vQjT5TFPD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having identified the most important variables from the model are the crime rate, average number of rooms and distance to employment, we can dig deeper into their marginal impacts and key interactions with other variables:"
      ]
    },
    {
      "metadata": {
        "id": "94rPFlVGTUsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "names = predictor_vars\n",
        "features = [7,5,0]\n",
        "fig, axs = plot_partial_dependence(gradient_boost, housing_train[predictor_vars], features,\n",
        "                                   feature_names=predictor_vars,\n",
        "                                   n_jobs=1, grid_resolution=50,\n",
        "                                  figsize = (20,5))\n",
        "fig.suptitle('Partial dependence plots for key features')\n",
        "plt.subplots_adjust(top=0.9) \n",
        "fig = plt.figure(figsize = (20,5))\n",
        "\n",
        "target_feature = (5, 0)\n",
        "pdp, axes = partial_dependence(gradient_boost, target_feature,\n",
        "                               X=housing_train[predictor_vars], grid_resolution=50)\n",
        "XX, YY = np.meshgrid(axes[0], axes[1])\n",
        "Z = pdp[0].reshape(list(map(np.size, axes))).T\n",
        "ax = Axes3D(fig)\n",
        "surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,\n",
        "                       cmap=plt.cm.BuPu, edgecolor='k')\n",
        "ax.set_xlabel(names[target_feature[0]])\n",
        "ax.set_ylabel(names[target_feature[1]])\n",
        "ax.set_zlabel('Partial dependence')\n",
        "ax.view_init(elev=3, azim=160)\n",
        "plt.colorbar(surf)\n",
        "plt.suptitle('Partial dependence of median value on Crime Rate and Average Rooms')\n",
        "plt.subplots_adjust(top=0.9)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}