{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroDecisionTrees_EDS.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "eszKqy-1Lqif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Equity Data Science Educational Series\n",
        "\n",
        "### Monthly Data Science Lecture Series Part 1:\n",
        "### An Introduction to Decision Trees, Random Forests, and Gradient Boosting Machines"
      ]
    },
    {
      "metadata": {
        "id": "nVaouvV5LzUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All examples from today's EDS educational session are included within this Jupyter Notebook. For those new to Jupyter Notebooks, code blocks can be executed by selecting the desired cell and pressing 'shift+enter', or clicking the 'play' button that appears at the upper left of the block.\n",
        "\n",
        "The ouput from executing the code contained within the cell will then appear immediately below the executed cell. Give the simple example below a try:"
      ]
    },
    {
      "metadata": {
        "id": "p_Ck01J1MH7B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Jupyter Test - Welcome to the EDS Educational Series on Wednesday, August\", 2**5-10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNZOXPCUoh00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Before we begin, please take a moment to install a visualization engine by executing the below cell:*"
      ]
    },
    {
      "metadata": {
        "id": "CjmEWyL-ogsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install graphviz \n",
        "!apt-get install graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LS7iPV4YMSCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "### The following topics will be discussed during today's session:\n",
        "\n",
        "    > Decision Tree Modeling\n",
        "        > Decision Tree Classification\n",
        "        > Decision Tree Regression\n",
        "    > Decision Tree Ensemble Methods\n",
        "        > Random Forest\n",
        "        > Gradient Boosting Machines"
      ]
    },
    {
      "metadata": {
        "id": "Xgdhjyq5Q5sV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code examples for today's session will utilize a dataset of median home prices across neighborhoods in the Boston Metropolitan area that is a common benchmark for algorithms in the machine learning community. Let's take a look at the data:"
      ]
    },
    {
      "metadata": {
        "id": "zeXkYS95RBg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# load dataset\n",
        "boston = load_boston()\n",
        "housing = pd.DataFrame(boston.data)\n",
        "housing['Median_Val'] = boston.target\n",
        "housing.columns = ['Crime_Rate', 'Prop_Zoned', 'Prop_NonRetail', 'Borders_River',\n",
        "                   'Conc_NO2', 'Avg_Rooms','Prop_Pre1940', 'Dist_Employ', \n",
        "                   'Hway_Access', 'Property_Tax', 'TeachStu_Ratio', 'B', \n",
        "                   'LSTAT', 'Median_Val']\n",
        "\n",
        "# Dropping a few columns to reduce dimensions\n",
        "drop_cols = ['B', 'LSTAT', 'Property_Tax']\n",
        "housing = housing[[a_col for a_col in housing.columns if not a_col in drop_cols]]\n",
        "\n",
        "# Print first 10 rows of the housing data set\n",
        "housing.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcygicgiRmh3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to also utilize the housing data to explore the application of decision trees to classification problems, we will add a categorical target that splits the median value into terciles for use in our decision tree classifiers:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zZeR5nyQRrb8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create categorical version of response variable [0 = lowest, 1 = middle, 2 = high]\n",
        "housing['Val_Cat'] = pd.qcut(housing.Median_Val,3,labels = [0,1,2])\n",
        "housing.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-CuX1HwR33Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As our objective is to build a predictive model that will work out of sample, let's split the dataset into a training, validation, and test set to gauge predictive accuracy."
      ]
    },
    {
      "metadata": {
        "id": "ppHraef5RrbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summary statistics for variables\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into two groups: 80% train + validation, 20% test\n",
        "housing_train, housing_test = train_test_split(housing, test_size = 0.2, random_state = 1991)\n",
        "\n",
        "# split train data into two further groups (train + validation)\n",
        "housing_train, housing_valid = train_test_split(housing_train, test_size = 0.2, random_state = 1991)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_od_kp8eR-_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We now have three data sets, 323 training data points, 81 validation data points, and 102 test data points\n",
        "print(\"Training Data: Number of Rows, Number of Columns:\", housing_train.shape)\n",
        "print(\"Validation Data: Number of Rows, Number of Columns:\",housing_valid.shape)\n",
        "print(\"Testing Data: Number of Rows, Number of Columns:\",housing_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWj1XHWyR8NS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a quick peak at the summary statistics and correlations among variables in the training data:"
      ]
    },
    {
      "metadata": {
        "id": "2-CllfNESJRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# display summary statistics for training data\n",
        "housing_train.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYCPxo6dStwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# correlation matrix of variables\n",
        "import seaborn as sborn\n",
        "color_map= sborn.diverging_palette(5, 250, as_cmap=True)\n",
        "feature_cor = housing_train.corr()\n",
        "feature_cor.style.background_gradient(color_map, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5YyDECpS7FW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before beginning any predictive modeling exercise, it is useful to:\n",
        "\n",
        "    > Establish Measure of Model Performance\n",
        "      > Mean Squared Error\n",
        "    > Generate Lazy Performance Baseline\n",
        "      > Average Value\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "q9NdGLL4S-0k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function to compute mean squared error\n",
        "def mean_squared_error(predicted_values, actual_values):\n",
        "    return np.mean((predicted_values - actual_values) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJNArDcnTGVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Decision Tree Modeling"
      ]
    },
    {
      "metadata": {
        "id": "zzCzPwLXTIKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Decision trees are a class of nonpametric models that partition datasets into sub-regions.\n",
        "\n",
        "In predictive modeling tasks, predictions for new observations are generated by following the rules of the tree from root to leaf.\n",
        "\n",
        "Let's take a look at a quick example:"
      ]
    },
    {
      "metadata": {
        "id": "fzOBpMyWTLBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import decision tree building functions and visualization functions from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import display, SVG\n",
        "from graphviz import Source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xueGkuK4USd6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fit a basic decision tree classifier on the training data\n",
        "import os\n",
        "\n",
        "ind_vars =['Avg_Rooms', 'Dist_Employ']\n",
        "classifier = DecisionTreeClassifier(max_depth = 2)\n",
        "classifier.fit(housing_train.head(99)[ind_vars], housing_train.head(99).Val_Cat)\n",
        "\n",
        "# Plot the tree\n",
        "tree_plot = Source(tree.export_graphviz(classifier, out_file = None, \n",
        "                        feature_names = ind_vars, class_names = ['0', '1', '2'],\n",
        "                        filled = True))\n",
        "display(SVG(tree_plot.pipe(format = 'svg')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcMAjVFnUh-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# subset of the training data to make visualization more manageable\n",
        "train_sub1 = housing_train.sample(n=150, random_state=1991, replace=False)\n",
        "train_sub1_x = train_sub1[['Avg_Rooms', 'Dist_Employ']]\n",
        "train_sub1_y = train_sub1.Val_Cat\n",
        "plt.scatter(train_sub1_x.Avg_Rooms, train_sub1_x.Dist_Employ, c=train_sub1_y, cmap = 'rainbow')\n",
        "plt.title('Average Rooms vs. Distance to Employment')\n",
        "plt.xlabel('Average Rooms')\n",
        "plt.ylabel('Distance to Employment')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jB7D-JeveFEh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tree(max_depth, min_leaf = 0.2):\n",
        "    estimator = DecisionTreeClassifier(random_state = 1991,\n",
        "                                      max_depth = max_depth,\n",
        "                                      min_samples_leaf = min_leaf)\n",
        "    estimator = estimator.fit(train_sub2_x,train_sub2_y)\n",
        "    \n",
        "    graph = Source(tree.export_graphviz(estimator, out_file = None, feature_names = train_sub2_x.columns, \n",
        "                                       class_names = ['0', '1', '2'],\n",
        "                                       filled = True))\n",
        "    #plot_tree_boundaries(max_depth, train_sub2_x, train_sub2_y)\n",
        "    display(SVG(graph.pipe(format = 'svg')))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwgeMg1-eK8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Try changing the tree tuning parameters. { run: \"auto\" }\n",
        "max_depth = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "min_leaf = 0.21 #@param {type:\"slider\", min:0.01, max:1, step:0.05}\n",
        "\n",
        "plot_tree(max_depth, min_leaf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mTj6gqRMUndE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tree_boundaries(max_depth, data_x, data_y, par = None, cmap = 'rainbow'):\n",
        "    if par is None:\n",
        "        par = plt.gca()\n",
        "        \n",
        "    # plot the training points\n",
        "    par.scatter(data_x.values[:,0], data_x.values[:,1], c = data_y, s = 50, cmap = cmap, clim = (data_y.min(), data_y.max()), zorder = 3)\n",
        "    par.axis('tight')\n",
        "    par.axis('off')\n",
        "    lim_x = par.get_xlim()\n",
        "    lim_y = par.get_ylim()\n",
        "    \n",
        "    classifier = DecisionTreeClassifier(max_depth = max_depth)\n",
        "    classifier.fit(data_x, data_y)\n",
        "    mesh_x, mesh_y = np.meshgrid(np.linspace(*lim_x, num = 200), np.linspace(*lim_y, num = 200))\n",
        "    mesh_preds = classifier.predict(np.c_[mesh_x.ravel(), mesh_y.ravel()]).reshape(mesh_x.shape)\n",
        "    \n",
        "    n_class = len(np.unique(data_y))\n",
        "    mesh_colors = par.contourf(mesh_x, mesh_y, mesh_preds, alpha = 0.3, levels = np.arange(n_class + 1)-0.5, \n",
        "                               cmap=cmap, clim = (data_y.min(), data_y.max()), zorder = 1)\n",
        "    \n",
        "    par.set(xlim = lim_x, ylim = lim_y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9He04W7SUroO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_tree_boundaries(8, train_sub1_x, train_sub1_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPD6AgX7U0Cc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# subset of the training data to make visualization more manageable\n",
        "train_sub2 = housing_train.sample(n=150, random_state=1992, replace=False)\n",
        "train_sub2_x = train_sub2[['Avg_Rooms', 'Dist_Employ']]\n",
        "train_sub2_y = train_sub2.Val_Cat\n",
        "plot_tree_boundaries(8, train_sub2_x, train_sub2_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edPjCcgWhr9z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Decision Tree Ensemble Methods\n",
        "\n",
        "## Random Forests"
      ]
    },
    {
      "metadata": {
        "id": "60a_SBwlhtaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhuRbfZthvUH",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "def plot_random_forest(n_estimators=1):\n",
        "            \n",
        "    data_x = train_sub2_x\n",
        "    data_y = train_sub2_y\n",
        "    par = None\n",
        "    cmap = 'rainbow'\n",
        "    max_depth = 1\n",
        "    min_leaf = 0.2\n",
        "    \n",
        "    if par is None:\n",
        "        par = plt.gca()\n",
        "\n",
        "    par.scatter(data_x.values[:,0], data_x.values[:,1], c = data_y, s = 50, cmap = cmap, clim = (data_y.min(), data_y.max()), zorder = 3)\n",
        "    par.axis('tight')\n",
        "    par.axis('off')\n",
        "    lim_x = par.get_xlim()\n",
        "    lim_y = par.get_ylim()\n",
        "    \n",
        "    rf_estimator = RandomForestClassifier(random_state = 1991,\n",
        "                                      n_estimators = n_estimators,\n",
        "                                      min_samples_leaf =2)\n",
        "    rf_estimator.fit(train_sub2_x,train_sub2_y)\n",
        "    \n",
        "    mesh_x, mesh_y = np.meshgrid(np.linspace(*lim_x, num = 200), np.linspace(*lim_y, num = 200))\n",
        "    mesh_preds = rf_estimator.predict(np.c_[mesh_x.ravel(), mesh_y.ravel()]).reshape(mesh_x.shape)\n",
        "    \n",
        "    n_class = len(np.unique(data_y))\n",
        "    mesh_colors = par.contourf(mesh_x, mesh_y, mesh_preds, alpha = 0.3, levels = np.arange(n_class + 1)-0.5, \n",
        "                               cmap=cmap, zorder = 1)\n",
        "    \n",
        "    par.set(xlim = lim_x, ylim = lim_y)    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5D5QTTv_sK8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Filler text"
      ]
    },
    {
      "metadata": {
        "id": "7oE97IzQsMj9",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the decision surface changes as we add more trees. { run: \"auto\" }\n",
        "n_estimators = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "plot_random_forest(n_estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9A8g4ISsMin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1nBnCwY9sK7i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "metadata": {
        "id": "5924ytvatThS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBzAhY0itXTC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# subset of the training data to make visualization more manageable\n",
        "boosting_subset = housing_train.sample(n=150, random_state=1991, replace=False)\n",
        "boosting_subset = boosting_subset.sort_values('Avg_Rooms')\n",
        "boosting_subset_x = boosting_subset['Avg_Rooms']\n",
        "boosting_subset_y = boosting_subset['Median_Val']\n",
        "plt.scatter(boosting_subset_x, boosting_subset_y, c=boosting_subset_y, cmap = 'rainbow')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AAc_lskRteXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_error = np.empty(100) * np.nan\n",
        "valid_error = np.empty(100) * np.nan\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iKnmmEGtXR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_gradient_boosting(num_trees=1, learn_rate = 0.30):\n",
        "    \n",
        "    data_x = boosting_subset_x.values.reshape(-1, 1)\n",
        "    data_y = boosting_subset_y\n",
        "    cmap = 'rainbow'\n",
        "\n",
        "    gradient_boosting_estimator = GradientBoostingRegressor(random_state = 1991,\n",
        "                                      n_estimators = num_trees,\n",
        "                                      learning_rate = learn_rate,\n",
        "                                      min_samples_leaf = 2,\n",
        "                                      max_depth = 1)\n",
        "    gradient_boosting_estimator = gradient_boosting_estimator.fit(data_x, data_y)\n",
        "\n",
        "    gradient_boosting_predictions = gradient_boosting_estimator.predict(data_x)\n",
        "    gradient_boosting_residuals = data_y - gradient_boosting_predictions\n",
        "    \n",
        "    # generate predictions on validation dataset\n",
        "    validation_predictions = gradient_boosting_estimator.predict(housing_valid['Avg_Rooms'].values.reshape(-1,1))\n",
        "    train_mse = mean_squared_error(boosting_subset_y, gradient_boosting_predictions)\n",
        "    valid_mse = mean_squared_error(housing_valid['Median_Val'].values, validation_predictions)\n",
        "    train_error[num_trees-1] = train_mse\n",
        "    valid_error[num_trees-1] = valid_mse\n",
        "    \n",
        "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, figsize = (20,5))\n",
        "    ax1.scatter(data_x, gradient_boosting_predictions, c = gradient_boosting_predictions, s = 50, cmap = cmap)\n",
        "    ax1.set_title('Gradient Boosting Predicted Values - ' +  str(num_trees) + ' iteration(s)')\n",
        "    ax1.axis((3.5,8.5,0,55))\n",
        "    ax2.scatter(data_x, gradient_boosting_residuals, c = np.abs(gradient_boosting_residuals), s = 50, cmap = cmap)\n",
        "    ax2.set_title('Gradient Boosting Residuals - ' +  str(num_trees) + ' iteration(s)')\n",
        "    ax2.axis((3.5,8.5,-30,30))\n",
        "    ax3.plot(range(0,num_trees), train_error[0:num_trees], label = 'train')\n",
        "    ax3.plot(range(0,num_trees), valid_error[0:num_trees], label = 'valid')\n",
        "    ax3.set_title('Gradient Boosting Validation Error - ' + str(num_trees) + ' iteration(s)')\n",
        "    ax3.legend(loc = 'upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45K9vx10tjbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the decision surface changes as we add more trees. { run: \"auto\" }\n",
        "num_trees = 23 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "learn_rate = 0.3 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "\n",
        "plot_gradient_boosting(num_trees, learn_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YuxAgnA0vw6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Minimum error of ', np.nanmin(valid_error), ' reached after ', np.nanargmin(valid_error)+1, 'iterations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rcXQr7KYtjaA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_error = np.empty(100) * np.nan\n",
        "valid_error = np.empty(100) * np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOH8jOtLxNak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the decision surface changes as we add more trees. { run: \"auto\" }\n",
        "num_trees = 10 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "learn_rate = 0.96 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "\n",
        "plot_gradient_boosting(num_trees, learn_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOyJ8ahcxndm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Minimum error of ', np.nanmin(valid_error), ' reached after ', np.nanargmin(valid_error)+1, 'iterations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2ktnW75yH9I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_error = np.empty(100) * np.nan\n",
        "valid_error = np.empty(100) * np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OdHNZPTeyG6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Watch how the decision surface changes as we add more trees. { run: \"auto\" }\n",
        "num_trees = 9 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "learn_rate = 0.05 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "\n",
        "plot_gradient_boosting(num_trees, learn_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H5bnbsMGsacd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparison of Model Performance\n"
      ]
    },
    {
      "metadata": {
        "id": "m3Z5IDkQsfeE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that the underlying model approaches have been introduced, we will now select the 'best' model by analyzing performance on the validation data set. This champion model will then be utilized to generate predictions on the unseen test data set for a more complete picture of 'out of sample' performance."
      ]
    },
    {
      "metadata": {
        "id": "SqyMQRpdw7dR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictor_vars = ['Crime_Rate', 'Prop_Zoned', 'Prop_NonRetail', 'Borders_River',\n",
        "                   'Conc_NO2', 'Avg_Rooms','Prop_Pre1940', 'Dist_Employ', \n",
        "                   'Hway_Access', 'TeachStu_Ratio']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "agIh1429t3Y9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Decision Trees with Different Maximum Depths / Minimum Leaf Sizes\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "maximum_tree_depths = [1,3,5,7,9]\n",
        "minimum_tree_leaf_sizes = [1,5,10,15]\n",
        "\n",
        "tree_results = pd.DataFrame(list(product(maximum_tree_depths, minimum_tree_leaf_sizes)), columns=['Max_Depth', 'Leaf_Size'])\n",
        "tree_results['MSE'] = np.NAN\n",
        "\n",
        "tree_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xRP7185wC7r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for a_row in range(len(tree_results)):\n",
        "  \n",
        "  # build a decision tree\n",
        "  decision_tree = DecisionTreeRegressor(max_depth = tree_results['Max_Depth'][a_row],\n",
        "                                       min_samples_leaf = tree_results['Leaf_Size'][a_row],\n",
        "                                       random_state = 1991)\n",
        "  \n",
        "  # fit the decision tree\n",
        "  decision_tree = decision_tree.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "  # generate predictions\n",
        "  tree_predictions = decision_tree.predict(housing_valid[predictor_vars])\n",
        "  # generate error\n",
        "  tree_error = mean_squared_error(tree_predictions, housing_valid['Median_Val'])\n",
        "  # fill in column in test results\n",
        "  tree_results.loc[a_row, 'MSE'] = tree_error\n",
        "  \n",
        "tree_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7L1anBQv5PwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Random Forests with Different Maximum Depths / Minimum Leaf Sizes / Number of Trees\n",
        "\n",
        "maximum_rf_depths = [5,1000]\n",
        "minimum_rf_leaf_sizes = [1,5,10]\n",
        "number_rf_trees = [50,100]\n",
        "\n",
        "rf_results = pd.DataFrame(list(product(maximum_rf_depths, minimum_rf_leaf_sizes, number_rf_trees)), columns=['Max_Depth', 'Leaf_Size', 'Num_Trees'])\n",
        "rf_results['MSE'] = np.NAN\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfVOCa4M0S3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for a_row in range(len(rf_results)):\n",
        "  \n",
        "  # build a random forest\n",
        "  random_forest = RandomForestRegressor(max_depth = rf_results['Max_Depth'][a_row],\n",
        "                                       min_samples_leaf = rf_results['Leaf_Size'][a_row],\n",
        "                                       n_estimators = rf_results['Num_Trees'][a_row],\n",
        "                                       random_state = 1991)\n",
        "  \n",
        "  # fit the random forest\n",
        "  random_forest = random_forest.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "  # generate predictions\n",
        "  rf_predictions = random_forest.predict(housing_valid[predictor_vars])\n",
        "  # generate error\n",
        "  rf_error = mean_squared_error(rf_predictions, housing_valid['Median_Val'])\n",
        "  # fill in column in test results\n",
        "  rf_results.loc[a_row, 'MSE'] = rf_error\n",
        "  \n",
        "rf_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tTN8Y3EE7rS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Gradient Boosting with Different Maximum Depths / Number of Trees / Learn Rates\n",
        "\n",
        "maximum_gb_depths = [1,7]\n",
        "number_gb_trees = [30,300]\n",
        "gb_learn_rate = [.05,.3,]\n",
        "\n",
        "gb_results = pd.DataFrame(list(product(maximum_gb_depths, number_gb_trees, gb_learn_rate)), columns=['Max_Depth', 'Num_Trees', 'Learn_Rate'])\n",
        "gb_results['MSE'] = np.NAN\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBu7ULd588Xx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for a_row in range(len(gb_results)):\n",
        "  \n",
        "  # build a gradient booster\n",
        "  gradient_boost = GradientBoostingRegressor(max_depth = gb_results['Max_Depth'][a_row],\n",
        "                                       n_estimators = gb_results['Num_Trees'][a_row],\n",
        "                                       learning_rate = gb_results['Learn_Rate'][a_row],\n",
        "                                       min_samples_leaf = 5,\n",
        "                                       random_state = 1991)\n",
        "  \n",
        "  # fit the gradient booster\n",
        "  gradient_boost = gradient_boost.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "  # generate predictions\n",
        "  gb_predictions = gradient_boost.predict(housing_valid[predictor_vars])\n",
        "  # generate error\n",
        "  gb_error = mean_squared_error(gb_predictions, housing_valid['Median_Val'])\n",
        "  # fill in column in test results\n",
        "  gb_results.loc[a_row, 'MSE'] = gb_error\n",
        "  \n",
        "gb_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0kaBCWL0EL5P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now retrain each model type utilizing the optimal parameters identified in the validation procedure above. "
      ]
    },
    {
      "metadata": {
        "id": "WLm1PUK5-Itq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# decision tree\n",
        "decision_tree = DecisionTreeRegressor(max_depth = 5,\n",
        "                                     min_samples_leaf = 5,\n",
        "                                     random_state = 1991)\n",
        "decision_tree = decision_tree.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "\n",
        "# random forest\n",
        "random_forest = RandomForestRegressor(max_depth = 1000,\n",
        "                                     min_samples_leaf = 1,\n",
        "                                     n_estimators = 100,\n",
        "                                     random_state = 1991)\n",
        "random_forest = random_forest.fit(housing_train[predictor_vars], housing_train['Median_Val'])\n",
        "\n",
        "# gradient boosting\n",
        "gradient_boost = GradientBoostingRegressor(max_depth = 7,\n",
        "                                     n_estimators = 300,\n",
        "                                     learning_rate = 0.05,\n",
        "                                     random_state = 1991,\n",
        "                                     min_samples_leaf = 5)\n",
        "\n",
        "# fit the gradient booster\n",
        "gradient_boost = gradient_boost.fit(housing_train[predictor_vars], housing_train['Median_Val'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66aXbY12EWPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now generate predictions (along with a simple 'average' baseline) to evaluate how well the 'best' models identified through the validation procedure perform on the unseen test data."
      ]
    },
    {
      "metadata": {
        "id": "z5pgNNG8EzSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# simple average predictions\n",
        "avg_predictions = [housing_train['Median_Val'].mean()] * len(housing_test)\n",
        "# decision tree predictions\n",
        "tree_predictions = decision_tree.predict(housing_test[predictor_vars])\n",
        "# random forest predictions\n",
        "rf_predictions = random_forest.predict(housing_test[predictor_vars])\n",
        "# gradient boosting predictions\n",
        "gb_predictions = gradient_boost.predict(housing_test[predictor_vars])\n",
        "\n",
        "\n",
        "# errors across the models\n",
        "avg_error = mean_squared_error(avg_predictions, housing_test['Median_Val'])\n",
        "tree_error = mean_squared_error(tree_predictions, housing_test['Median_Val'])\n",
        "rf_error = mean_squared_error(rf_predictions, housing_test['Median_Val'])\n",
        "gb_error = mean_squared_error(gb_predictions, housing_test['Median_Val'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FoybXZRcGaT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Test Error Using Average from Training Set:', avg_error)\n",
        "print('Test Error Using Decision Tree:',tree_error)\n",
        "print('Test Error Using Random Forest:',rf_error)\n",
        "print('Test Error Using Gradient Boosting:',gb_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgXuG8-0N--v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Barplot\n",
        "result_height = [avg_error, tree_error, rf_error, gb_error]\n",
        "result_bars = ('Average', 'Tree', 'RandomForest', 'GradientBoosting')\n",
        "\n",
        "# Create bars\n",
        "plt.bar(result_bars, result_height)\n",
        "\n",
        "# add in a title\n",
        "plt.title('Test Error by Model' )\n",
        "plt.ylabel('Mean Squared Error')\n",
        "\n",
        "# Show graphic\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6KG6HzRQTPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extracting Insight from the Selected Model"
      ]
    },
    {
      "metadata": {
        "id": "S3JmVDC_Q2Cj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have selected our champion model and confirmed its efficacy utilizing the out of sample test data, we will attempt to leverage additional tools to better understand the insights that drive its predictions."
      ]
    },
    {
      "metadata": {
        "id": "MDG7pGiHRlen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# variable importances from model\n",
        "gb_importances = gradient_boost.feature_importances_\n",
        "\n",
        "# Plot the feature importances of the ensemble\n",
        "plt.figure()\n",
        "plt.title(\"Gradient Boosting Feature importances\")\n",
        "plt.barh(predictor_vars, gb_importances, color=\"r\", align=\"center\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "pd.DataFrame({'Feature' : predictor_vars, 'Importance': gb_importances}).sort_values(\"Importance\", ascending = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_L1vQjT5TFPD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having identified the most important variables from the model, we can dig deeper into their marginal impacts and key interactions with other variables:"
      ]
    },
    {
      "metadata": {
        "id": "94rPFlVGTUsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "names = predictor_vars\n",
        "features = [7,5,0]\n",
        "fig, axs = plot_partial_dependence(gradient_boost, housing_train[predictor_vars], features,\n",
        "                                   feature_names=predictor_vars,\n",
        "                                   n_jobs=1, grid_resolution=50,\n",
        "                                  figsize = (20,5))\n",
        "fig.suptitle('Partial dependence plots for key features')\n",
        "plt.subplots_adjust(top=0.9) \n",
        "fig = plt.figure(figsize = (20,5))\n",
        "\n",
        "target_feature = (5, 0)\n",
        "pdp, axes = partial_dependence(gradient_boost, target_feature,\n",
        "                               X=housing_train[predictor_vars], grid_resolution=50)\n",
        "XX, YY = np.meshgrid(axes[0], axes[1])\n",
        "Z = pdp[0].reshape(list(map(np.size, axes))).T\n",
        "ax = Axes3D(fig)\n",
        "surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,\n",
        "                       cmap=plt.cm.BuPu, edgecolor='k')\n",
        "ax.set_xlabel(names[target_feature[0]])\n",
        "ax.set_ylabel(names[target_feature[1]])\n",
        "ax.set_zlabel('Partial dependence')\n",
        "ax.view_init(elev=3, azim=160)\n",
        "plt.colorbar(surf)\n",
        "plt.suptitle('Partial dependence of median value on Crime Rate and Average Rooms')\n",
        "plt.subplots_adjust(top=0.9)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}